{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_good_cl_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekadithya-s/Computational_Linguistics/blob/master/colab_good_cl_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GpcnAbM4p0vh",
        "colab_type": "code",
        "outputId": "8517b53f-3d41-4153-c281-8baaa70f875a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PoGKQq9_a19C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **OFFENSIVE WORD DETECTION** "
      ]
    },
    {
      "metadata": {
        "id": "-CTo6aKbDYJQ",
        "colab_type": "code",
        "outputId": "236bd668-e060-4f09-96fc-ce2767a3ade2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "dafa = pd.read_csv('drive/My Drive/Colab Notebooks/CLData/labeled_data.csv')\n",
        "\n",
        "#<p>The dataset was used in Proceedings of the 11th International AAAI Conference on Weblogs and Social Media</p>\n",
        "#The dataset is obtained from this website\n",
        "#https://data.world/thomasrdavidson/hate-speech-and-offensive-language\n",
        "\n",
        "#<p>Every tweet in this dataset is marked either as hate_speech, offensive_lanuage or neither</p>\n",
        "\n",
        "#We have considered hate_speech and offensive_language as one category and neither as another category\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "dafa = dafa.drop('count', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('hate_speech', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('offensive_language', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('neither', axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "dafa = dafa.drop(dafa.columns[0], axis = 1)\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "y = dafa['class']\n",
        "\n",
        "dafa = dafa.drop('class', axis = 1)\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "#dafa = dafa = dafa.drop('neither', axis = 1)\n",
        "\n",
        "#y.head(5)\n",
        "\n",
        "y = y.replace(to_replace = 1, value = 0)\n",
        "\n",
        "y = y.replace(to_replace = 2, value = 1)\n",
        "\n",
        "#dafa = dafa.drop('class', axis = 1)\n",
        "\n",
        "df = dafa\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], y, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "#clf = MultinomialNB()\n",
        "clf = PassiveAggressiveClassifier()\n",
        "clf.fit(tfidf_train, y_train)\n",
        "pred = clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Offensive word detection accuracy using PAC:   %0.3f\" % score)\n",
        "\n",
        "#y_test.head(5)\n",
        "\n",
        "#pred\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==1):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==0):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Offensive word detection accuracy using PAC:   94.669\n",
            "[[6607  193]\n",
            " [ 243 1136]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_AsNzDsw9mcO",
        "colab_type": "code",
        "outputId": "6edff89e-573d-49cc-a92a-9f966e1a60fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(tfidf_train, y_train)\n",
        "pred = clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Offensive word detection accuracy using MNB:   %0.3f\" % score)\n",
        "\n",
        "#y_test.head(5)\n",
        "\n",
        "#pred\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==1):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==0):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Offensive word detection accuracy using MNB:   84.705\n",
            "[[6796    4]\n",
            " [1247  132]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TZWDu2P7DYMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pw3XYcILDYMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# FAKE NEWS DETECTION"
      ]
    },
    {
      "metadata": {
        "id": "I74xxyI3DYNS",
        "colab_type": "code",
        "outputId": "f2bc62c9-3ae9-4c74-8368-84b23eaf7d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/Colab Notebooks/CLData/train.tsv', delimiter='\\t')\n",
        "print(df.shape)\n",
        "\n",
        "#This is the Liar, Liar pants on Fire dataset by William Yang Wang\n",
        "#https://arxiv.org/pdf/1705.00648.pdf\n",
        "#This dataset divides the news into six categories - <b>Half-true, mostly-true, true, barely-true, false, pants-fire</b></p>\n",
        "#For our project we have reduced the labesl to two - Fake or Real. This means we have combined similar labels.</p> \n",
        "\n",
        "\n",
        "\n",
        "new = df[df.columns[1:3]]\n",
        "\n",
        "\n",
        "\n",
        "new.columns = ['Label', 'news']\n",
        "\n",
        "\n",
        "\n",
        "new['Label'].unique()\n",
        "\n",
        "#new['Label'] = new['Label'].replace('half-true','true')\n",
        "#new['Label'] = new['Label'].replace('mostly-true','true')\n",
        "#new['Label'] = new['Label'].replace('barely-true','false')\n",
        "new['Label'] = new['Label'].replace('pants-fire','false')\n",
        "\n",
        "train = new.copy()\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/Colab Notebooks/CLData/test.tsv', delimiter='\\t')\n",
        "new2 = df[df.columns[1:3]]\n",
        "new2.columns = ['Label', 'news']\n",
        "#new2['Label'] = new2['Label'].replace('half-true','true')\n",
        "#new2['Label'] = new2['Label'].replace('mostly-true','true')\n",
        "#new2['Label'] = new2['Label'].replace('barely-true','false')\n",
        "new2['Label'] = new2['Label'].replace('pants-fire','false')\n",
        "test = new2.copy()\n",
        "\n",
        "frames = [train, test]\n",
        "result = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "new['Label'].value_counts()\n",
        "\n",
        "new1 = new[new.Label=='true'].sample(n=1000).copy()\n",
        "new2 = new[new.Label=='false'].sample(frac=1).copy()\n",
        "frames = [new1, new2]\n",
        "result = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "\n",
        "result['Label'].value_counts()\n",
        "\n",
        "result = result.sample(frac=1)\n",
        "df = result\n",
        "\n",
        "df['Label'].value_counts()\n",
        "\n",
        "\n",
        "\n",
        "y = df['Label']\n",
        "df = df.drop('Label', axis = 1)\n",
        "\n",
        "#df.shape\n",
        "\n",
        "maxacc=0\n",
        "for iii in range(1):\n",
        "    #df = df.sample(frac=1)\n",
        "    #print(df['Label'].value_counts())\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['news'], y, test_size=0.20, random_state=24)\n",
        "    #iii+=9\n",
        "    count_vectorizer = CountVectorizer(stop_words='english')\n",
        "    count_train = count_vectorizer.fit_transform(X_train)\n",
        "    count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "    tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    tfidf_vectorizer.get_feature_names()[-10:]\n",
        "    count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "    count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "    tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "    difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "    #print(count_df.equals(tfidf_df))\n",
        "    count_df.head()\n",
        "\n",
        "    tfidf_df.head()\n",
        "\n",
        "    linear_clf = MultinomialNB()\n",
        "    #linear_clf = PAC(max_iter=1000)\n",
        "\n",
        "\n",
        "    linear_clf.fit(tfidf_train, y_train)\n",
        "    pred = linear_clf.predict(tfidf_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    score = score * 100\n",
        "    if(score>maxacc):\n",
        "        maxacc=score\n",
        "        print(\"Fake News detection accuracy using MNB:   %0.3f\" % score)\n",
        "        \n",
        "    cm = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
        "\n",
        "print(cm)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10239, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fake News detection accuracy using MNB:   75.359\n",
            "[[  0 189]\n",
            " [  0 578]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-uiueoidDYNY",
        "colab_type": "code",
        "outputId": "87624117-d965-4ad6-e22d-91a03b99aec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier as PAC\n",
        "\n",
        "for i in range(1):  \n",
        "    linear_clf = PAC()\n",
        "\n",
        "\n",
        "    linear_clf.fit(tfidf_train, y_train)\n",
        "    pred = linear_clf.predict(tfidf_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    score = score * 100\n",
        "    if(score>0):\n",
        "        maxacc=score\n",
        "        print(\"Fake News detection accuracy using PAC:   %0.3f\" % score)\n",
        "    cm = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
        "\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake News detection accuracy using PAC:   68.970\n",
            "[[ 57 136]\n",
            " [102 472]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1GBhwFlYDYNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SARCASM DETECTION"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rKswTh9ZEEd0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#/Users/vivad/Downloads/all/yelp_training_set/yelp_training_set_review.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-QgEHpLDYNt",
        "colab_type": "code",
        "outputId": "f156fa25-f798-411c-9c06-26bab31a88a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import csv\n",
        "\n",
        "txt_file1 = b\"drive/My Drive/Colab Notebooks/CLData/sarcasm_tweets_gaps.txt\"\n",
        "csv_file1 = b\"drive/My Drive/Colab Notebooks/CLData/sarcasm_good.csv\"\n",
        "txt_file2 = b\"drive/My Drive/Colab Notebooks/CLData/nonsarcasm_tweets_nogap.txt\"\n",
        "csv_file2 = b\"drive/My Drive/Colab Notebooks/CLData/nonsarcasm_good.csv\"\n",
        "in_txt1 = csv.reader(open(txt_file1, \"rt\", encoding='UTF-8'), delimiter = '\\n')\n",
        "out_csv1 = csv.writer(open(csv_file1, 'wt', encoding='UTF-8'))\n",
        "\n",
        "out_csv1.writerows(in_txt1)\n",
        "\n",
        "in_txt2 = csv.reader(open(txt_file2, \"rt\", encoding='UTF-8'), delimiter = '\\n')\n",
        "out_csv2 = csv.writer(open(csv_file2, 'wt', encoding='UTF-8'))\n",
        "\n",
        "out_csv2.writerows(in_txt2)\n",
        "\n",
        "df1 = pd.read_csv('drive/My Drive/Colab Notebooks/CLData/sarcasm_good.csv', header=None)\n",
        "df2 = pd.read_csv('drive/My Drive/Colab Notebooks/CLData/nonsarcasm_good.csv', header=None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "frames = [df1, df2]\n",
        "df3 = pd.concat(frames, ignore_index=True)\n",
        "#print(df3.shape)\n",
        "df3[\"label\"] =''\n",
        "\n",
        "df3.shape\n",
        "for i in range(2303):\n",
        "    \n",
        "    df3.at[i,'label']=str('sarcasm')\n",
        "for i in range(2303, 3655):\n",
        "    \n",
        "    df3.at[i,'label']=str('non-sarcasm')\n",
        "df3 = df3.rename(columns={ df3.columns[0]: \"text\" })\n",
        "\n",
        "df3.shape\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "df3 = df3.rename(columns={ df3.columns[0]: \"text\" })\n",
        "\n",
        "df = df3.copy()\n",
        "dfnn = df.copy()\n",
        "df = df.sample(frac=1).copy()\n",
        "\n",
        "dforg = df.copy()\n",
        "#print(df.shape)\n",
        "#df = df.set_index('Unnamed: 0')\n",
        "dff = df.copy()\n",
        "df.head()\n",
        "y = df.label\n",
        "#dff = df.drop('title', axis=1)\n",
        "#dff = df.drop('text', axis=1)\n",
        "#dff =dff.drop(df.columns[[0,1,2]], axis=1).copy()\n",
        "#print(dff.head())\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop('label', axis=1)\n",
        "\n",
        "# Splitting the dataset into Training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=2)\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "X_trainpd = X_train.to_frame(name=None)\n",
        "\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "linear_clf = MultinomialNB()\n",
        "#linear_clf = PAC(max_iter=1000)\n",
        "\n",
        "\n",
        "linear_clf.fit(count_train, y_train)\n",
        "pred = linear_clf.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sarcasm Detection accuracy using MNB:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['sarcasm', 'non-sarcasm'])\n",
        "\n",
        "print(cm)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sarcasm Detection accuracy using MNB:   86.764\n",
            "[[718  62]\n",
            " [ 67 357]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UBdivnwvDYOc",
        "colab_type": "code",
        "outputId": "b9b3e292-53ee-45a8-ad7c-2b893bf621f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "linear_clf = PAC(max_iter=1000)\n",
        "\n",
        "\n",
        "linear_clf.fit(count_train, y_train)\n",
        "pred = linear_clf.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sarcasm Detection accuracy using PAC:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['sarcasm', 'non-sarcasm'])\n",
        "\n",
        "print(cm)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sarcasm Detection accuracy using PAC:   82.082\n",
            "[[718  58]\n",
            " [117 299]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7BlPTCIaDYOf",
        "colab_type": "code",
        "outputId": "ec460f38-7eea-467b-c290-0ed5b06c14ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "data = dfnn.copy()\n",
        "data = data.sample(frac=1).copy()\n",
        "df3[\"labelnn\"] =''\n",
        "\n",
        "data['labelnn'] = [1 if x == 'sarcasm' else 0 for x in data.label]\n",
        "from numpy.random import seed\n",
        "seed(13)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(13)\n",
        "\n",
        "X, y = (data['text'].values, data['labelnn'].values)\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(X)\n",
        "X_seq = tk.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=25, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.10)\n",
        "\n",
        "batch_size = 250\n",
        "X_train1 = X_train[batch_size:]\n",
        "y_train1 = y_train[batch_size:]\n",
        "X_valid = X_train[:batch_size]\n",
        "y_valid = y_train[:batch_size]\n",
        "\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 25\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adam = Adam(lr=0.0001)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train1, y_train1 , validation_data=(X_valid, y_valid), batch_size=40, epochs=7, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3128 samples, validate on 250 samples\n",
            "Epoch 1/7\n",
            "3128/3128 [==============================] - 17s 5ms/step - loss: 0.6197 - acc: 0.6646 - val_loss: 0.4054 - val_acc: 0.8240\n",
            "Epoch 2/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.2284 - acc: 0.9204 - val_loss: 0.2336 - val_acc: 0.9160\n",
            "Epoch 3/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.0909 - acc: 0.9763 - val_loss: 0.2633 - val_acc: 0.9200\n",
            "Epoch 4/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.0529 - acc: 0.9879 - val_loss: 0.3472 - val_acc: 0.8960\n",
            "Epoch 5/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.0312 - acc: 0.9936 - val_loss: 0.3497 - val_acc: 0.9040\n",
            "Epoch 6/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.0267 - acc: 0.9930 - val_loss: 0.2937 - val_acc: 0.9000\n",
            "Epoch 7/7\n",
            "3128/3128 [==============================] - 6s 2ms/step - loss: 0.0289 - acc: 0.9946 - val_loss: 0.3308 - val_acc: 0.9160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9839e01c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "N4kLK1uH-QJ2",
        "colab_type": "code",
        "outputId": "ca1a090b-e37e-4295-83a4-35d4cf70b424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy :\"+str(scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy :90.69148923488373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LdyS6NLjDYPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QZFanefTFYu1",
        "outputId": "3c54fd21-20ad-421a-9339-a99d285573bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gX9CeiTmgJk1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yza-hCoHgKJx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "487ESIWvEEeI",
        "outputId": "e6ceb128-931e-4497-d9eb-468f405e1f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn import metrics\n",
        "from keras.models import Sequential, model_from_json\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier as PAC\n",
        "from keras.layers.core import Dropout, Dense, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "#df=pd.read_json(\"drive/My Drive/Colab Notebooks/yelp_training_set_review.json\", lines=True)\n",
        "#print(df)\n",
        "#df.to_csv(\"drive/My Drive/Colab Notebooks/final_senti_results.csv\")\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/Colab Notebooks/final_senti_results.csv', index_col=False)\n",
        "dforg2 = df.copy()\n",
        "df.shape\n",
        "\n",
        "df['stars'].unique()\n",
        "\n",
        "dff =df.drop(df.columns[[0,1,2,3, 6,7,8]], axis=1).copy()\n",
        "\n",
        "dff.head()\n",
        "\n",
        "\n",
        "dff['stars'].unique()\n",
        "\n",
        "len(dff)\n",
        "\n",
        "dff['stars'].value_counts()\n",
        "\n",
        "dfpos = dff[dff.stars==5].sample(n=20000)\n",
        "\n",
        "dfpos.head()\n",
        "dfpos['stars'].value_counts()\n",
        "\n",
        "dfneg = dff[dff.stars==1].sample(frac=1)\n",
        "dfneg['stars'].value_counts()\n",
        "\n",
        "frames = [dfpos, dfneg]\n",
        "df3 = pd.concat(frames, ignore_index=True)\n",
        "df4 = df3.copy()\n",
        "df3['stars'] = df3['stars'].replace(1,'negative')\n",
        "df3['stars'] = df3['stars'].replace(5,'positive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2okMMLEgEEiG",
        "outputId": "45fc1ebf-d98c-4ddd-e7ba-c4ffba0b8058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "dforg = df3.copy()\n",
        "#df3['stars'] = df3['stars'].replace(1,'negative')\n",
        "#df3['stars'] = df3['stars'].replace(5,'positive')\n",
        "df = df3.copy()\n",
        "df = df.sample(n=20000).copy()\n",
        "\n",
        "#print(df.shape)\n",
        "#df = df.set_index('Unnamed: 0')\n",
        "dff = df.copy()\n",
        "#print(df.head())\n",
        "y = df.stars\n",
        "#dff = df.drop('title', axis=1)\n",
        "#dff = df.drop('text', axis=1)\n",
        "#dff =dff.drop(df.columns[[0,1,2]], axis=1).copy()\n",
        "#print(dff.head())\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop('stars', axis=1)\n",
        "\n",
        "# Splitting the dataset into Training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=2)\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "#linear_clf = MultinomialNB()\n",
        "linear_clf = PAC()\n",
        "\n",
        "\n",
        "linear_clf.fit(tfidf_train, y_train)\n",
        "pred = linear_clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sentiment detection using PAC - accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['positive', 'negative'])\n",
        "#print(cm)\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment detection using PAC - accuracy:   93.955\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.93      0.94      0.93      3056\n",
            "   positive       0.95      0.94      0.94      3544\n",
            "\n",
            "avg / total       0.94      0.94      0.94      6600\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ycborVCcw_aV",
        "colab_type": "code",
        "outputId": "4068c99e-9dba-4a8f-b2df-e43a255b0ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "linear_clf = MultinomialNB()\n",
        "#linear_clf = PAC()\n",
        "\n",
        "\n",
        "linear_clf.fit(tfidf_train, y_train)\n",
        "pred = linear_clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sentiment detection using MNB - accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['positive', 'negative'])\n",
        "#print(cm)\n",
        "print(classification_report(y_test, pred))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment detection using MNB - accuracy:   92.470\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.92      0.92      0.92      3056\n",
            "   positive       0.93      0.93      0.93      3544\n",
            "\n",
            "avg / total       0.92      0.92      0.92      6600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tyrrDP3cDYQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SENTIMENT DETECTION USING RNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6iujD3yXEEl1",
        "outputId": "1f0e4666-82c2-41ca-dc7f-6ea86578991e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "data = df4.copy()\n",
        "data = data.sample(n=20000).copy()\n",
        "data['Sentiment'] = [1 if x > 4 else 0 for x in data.stars]\n",
        "from numpy.random import seed\n",
        "seed(13)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(13)\n",
        "\n",
        "X, y = (data['text'].values, data['Sentiment'].values)\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(X)\n",
        "X_seq = tk.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=100, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.20)\n",
        "\n",
        "batch_size = 2000\n",
        "X_train1 = X_train[batch_size:]\n",
        "y_train1 = y_train[batch_size:]\n",
        "X_valid = X_train[:batch_size]\n",
        "y_valid = y_train[:batch_size]\n",
        "\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 100\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adam = Adam(lr=0.0001)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train1, y_train1 , validation_data=(X_valid, y_valid), batch_size=450, epochs=15, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14000 samples, validate on 2000 samples\n",
            "Epoch 1/15\n",
            "14000/14000 [==============================] - 12s 891us/step - loss: 0.6806 - acc: 0.5607 - val_loss: 0.6658 - val_acc: 0.6940\n",
            "Epoch 2/15\n",
            "14000/14000 [==============================] - 9s 673us/step - loss: 0.6225 - acc: 0.7108 - val_loss: 0.4215 - val_acc: 0.8270\n",
            "Epoch 3/15\n",
            "14000/14000 [==============================] - 9s 672us/step - loss: 0.2665 - acc: 0.8995 - val_loss: 0.2632 - val_acc: 0.9200\n",
            "Epoch 4/15\n",
            "14000/14000 [==============================] - 9s 672us/step - loss: 0.1126 - acc: 0.9616 - val_loss: 0.1822 - val_acc: 0.9380\n",
            "Epoch 5/15\n",
            "14000/14000 [==============================] - 9s 673us/step - loss: 0.0578 - acc: 0.9831 - val_loss: 0.1763 - val_acc: 0.9435\n",
            "Epoch 6/15\n",
            "14000/14000 [==============================] - 9s 673us/step - loss: 0.0335 - acc: 0.9919 - val_loss: 0.3305 - val_acc: 0.9185\n",
            "Epoch 7/15\n",
            "14000/14000 [==============================] - 9s 672us/step - loss: 0.0292 - acc: 0.9929 - val_loss: 0.2303 - val_acc: 0.9225\n",
            "Epoch 8/15\n",
            "14000/14000 [==============================] - 9s 672us/step - loss: 0.0270 - acc: 0.9934 - val_loss: 0.3976 - val_acc: 0.9155\n",
            "Epoch 9/15\n",
            "14000/14000 [==============================] - 9s 674us/step - loss: 0.0180 - acc: 0.9961 - val_loss: 0.3134 - val_acc: 0.9265\n",
            "Epoch 10/15\n",
            "14000/14000 [==============================] - 10s 691us/step - loss: 0.0110 - acc: 0.9978 - val_loss: 0.2091 - val_acc: 0.9315\n",
            "Epoch 11/15\n",
            "14000/14000 [==============================] - 9s 676us/step - loss: 0.0141 - acc: 0.9972 - val_loss: 0.6205 - val_acc: 0.9010\n",
            "Epoch 12/15\n",
            "14000/14000 [==============================] - 10s 682us/step - loss: 0.0204 - acc: 0.9961 - val_loss: 0.4502 - val_acc: 0.9110\n",
            "Epoch 13/15\n",
            "14000/14000 [==============================] - 9s 677us/step - loss: 0.0131 - acc: 0.9976 - val_loss: 0.5220 - val_acc: 0.9100\n",
            "Epoch 14/15\n",
            "14000/14000 [==============================] - 10s 681us/step - loss: 0.0138 - acc: 0.9969 - val_loss: 0.2611 - val_acc: 0.9245\n",
            "Epoch 15/15\n",
            "14000/14000 [==============================] - 9s 671us/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.3396 - val_acc: 0.9290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa12bf984e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fbBfFjf2EEl3",
        "outputId": "ef618d72-0a47-4529-cf53-ff57872b36fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy :\"+str(scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy :91.975\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
